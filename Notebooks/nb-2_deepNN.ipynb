{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXE3Hjh63ps_"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/retowuest/uio-dl-2024/blob/main/Notebooks/nb-2.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLu8EKyr3ptB"
      },
      "source": [
        "# Deep Learning for Social Scientists\n",
        "\n",
        "### University of Oslo, November 27-28, 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NCl2Spq3ptB"
      },
      "source": [
        "### **Demo 3:**<br>Fully Connected Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_Yc22bm3ptC"
      },
      "source": [
        "### Table of Contents\n",
        "* [Introduction](#section_1)\n",
        "* [Loading the Data](#section_2)\n",
        "* [Preprocessing the Data](#section_3)\n",
        "* [Training a DNN Regression Model](#section_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgAaM2EF3ptC"
      },
      "source": [
        "### Introduction <a class=\"anchor\" id=\"section_1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RAG5e6X3ptD"
      },
      "source": [
        "In this notebook, we will use [PyTorch](https://pytorch.org/) to build a feed-forward fully connected deep neural network to predict the fuel efficiency of a car in miles per gallon (MPG). Since MPG is a continuous variable, this is a regression task.\n",
        "\n",
        "We will use the **Auto MPG** data set, which is a common machine learning benchmark data set for predicting the fuel efficiency of a car in MPG. The data set and its description are available from the UC Irvine Machine Learning Repository [here](https://archive.ics.uci.edu/ml/datasets/auto+mpg).\n",
        "\n",
        "The data set includes 398 examples and nine features. We will use the following eight features *(data type in parentheses)*:\n",
        "\n",
        "- MPG *(continuous)*\n",
        "- Cylinders *(continuous)*\n",
        "- Displacement *(continuous)*\n",
        "- Horsepower *(continuous)*\n",
        "- Weight *(continuous)*\n",
        "- Acceleration *(continuous)*\n",
        "- Model Year *(ordered categorical)*\n",
        "- Origin *(unordered categorical)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdamREji3ptD"
      },
      "source": [
        "### Loading the Data <a class=\"anchor\" id=\"section_2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVomEXSZ3ptE"
      },
      "source": [
        "Let's first load the data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWbWLbwF3ptE"
      },
      "outputs": [],
      "source": [
        "# Import pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# Specify URL for downloading data set\n",
        "url = \"http://archive.ics.uci.edu/ml/\" \\\n",
        "      \"machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
        "\n",
        "# Create list with feature names\n",
        "col_names = [\n",
        "      \"MPG\", \"Cylinders\", \"Displacement\", \"Horsepower\",\n",
        "      \"Weight\", \"Acceleration\", \"Model Year\", \"Origin\"\n",
        "]\n",
        "\n",
        "# Load data set\n",
        "auto_mpg_df = pd.read_csv(\n",
        "      url, sep=\" \", names=col_names,\n",
        "      skipinitialspace=True,  # skip spaces after delimiter\n",
        "      na_values=\"?\",  # string to be recognized as NA\n",
        "      comment=\"\\t\"  # character indicating that remainder of line should not be parsed\n",
        ")\n",
        "\n",
        "# Convert column Cylinders from integer to float\n",
        "auto_mpg_df[\"Cylinders\"] = auto_mpg_df[\"Cylinders\"].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPykvwTk3ptF"
      },
      "source": [
        "### Preprocessing the Data <a class=\"anchor\" id=\"section_3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak9GXqB73ptF"
      },
      "source": [
        "As a first preprocessing step, we drop from the data set any rows containing NAs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvBYg_OG3ptG",
        "outputId": "4b956a2e-9ab7-4a37-92c5-81410d909983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPG             0\n",
            "Cylinders       0\n",
            "Displacement    0\n",
            "Horsepower      6\n",
            "Weight          0\n",
            "Acceleration    0\n",
            "Model Year      0\n",
            "Origin          0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Count number of NAs by column\n",
        "print(auto_mpg_df.isnull().sum(axis=0))\n",
        "\n",
        "# Drop rows with NAs\n",
        "auto_mpg_df = auto_mpg_df.dropna()\n",
        "auto_mpg_df = auto_mpg_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjiv5MLG3ptH"
      },
      "source": [
        "Next, we partition the data set into a training set and a test test, using the `train_test_split()` function from the `model_selection` module in the `scikit-learn` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xafcaba3ptH"
      },
      "outputs": [],
      "source": [
        "# Import scikit-learn library and the train_test_split function\n",
        "# from model_selection module in the scikit-learn library\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create training and test sets\n",
        "df_train, df_test = train_test_split(\n",
        "    auto_mpg_df,\n",
        "    train_size=0.8,  # indicates proportion of data set to include in training set\n",
        "    random_state=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LhGwPsr3ptH"
      },
      "source": [
        "We standardize the continuous inputs (which are `Cylinders`, `Displacement`, `Horsepower`, `Weight`, and `Acceleration`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEMcPI7h3ptH",
        "outputId": "02443f40-39cd-4152-a775-be0cdcafee13"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MPG</th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "      <th>Origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>28.0</td>\n",
              "      <td>-0.824303</td>\n",
              "      <td>-0.901020</td>\n",
              "      <td>-0.736562</td>\n",
              "      <td>-0.950031</td>\n",
              "      <td>0.255202</td>\n",
              "      <td>76</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>19.4</td>\n",
              "      <td>0.351127</td>\n",
              "      <td>0.413800</td>\n",
              "      <td>-0.340982</td>\n",
              "      <td>0.293190</td>\n",
              "      <td>0.548737</td>\n",
              "      <td>78</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>13.0</td>\n",
              "      <td>1.526556</td>\n",
              "      <td>1.144256</td>\n",
              "      <td>0.713897</td>\n",
              "      <td>1.339617</td>\n",
              "      <td>-0.625403</td>\n",
              "      <td>72</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>30.5</td>\n",
              "      <td>-0.824303</td>\n",
              "      <td>-0.891280</td>\n",
              "      <td>-1.053025</td>\n",
              "      <td>-1.072585</td>\n",
              "      <td>0.475353</td>\n",
              "      <td>77</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>14.0</td>\n",
              "      <td>1.526556</td>\n",
              "      <td>1.563051</td>\n",
              "      <td>1.636916</td>\n",
              "      <td>1.470420</td>\n",
              "      <td>-1.359240</td>\n",
              "      <td>71</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      MPG  Cylinders  Displacement  Horsepower    Weight  Acceleration  \\\n",
              "203  28.0  -0.824303     -0.901020   -0.736562 -0.950031      0.255202   \n",
              "255  19.4   0.351127      0.413800   -0.340982  0.293190      0.548737   \n",
              "72   13.0   1.526556      1.144256    0.713897  1.339617     -0.625403   \n",
              "235  30.5  -0.824303     -0.891280   -1.053025 -1.072585      0.475353   \n",
              "37   14.0   1.526556      1.563051    1.636916  1.470420     -1.359240   \n",
              "\n",
              "     Model Year  Origin  \n",
              "203          76       3  \n",
              "255          78       1  \n",
              "72           72       1  \n",
              "235          77       1  \n",
              "37           71       1  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create list with names of numeric features\n",
        "numeric_col_names = [\n",
        "    \"Cylinders\", \"Displacement\",\n",
        "    \"Horsepower\", \"Weight\",\n",
        "    \"Acceleration\"\n",
        "]\n",
        "\n",
        "# Use describe method to obtain descriptive statistics for features\n",
        "# (we will use the mean and sd to standardize the continuous features)\n",
        "train_stats = df_train.describe().transpose()\n",
        "\n",
        "# Create copies of the training and test sets\n",
        "df_train_std, df_test_std = df_train.copy(), df_test.copy()\n",
        "\n",
        "# Iterate over numeric inputs and standardize them\n",
        "for col_name in numeric_col_names:\n",
        "    mean = train_stats.loc[col_name, \"mean\"]  # we use the loc method to access rows and columns by label\n",
        "    sd = train_stats.loc[col_name,\"std\"]\n",
        "    df_train_std.loc[:, col_name] = (df_train_std.loc[:, col_name] - mean) / sd\n",
        "    df_test_std.loc[:, col_name] = (df_test_std.loc[:, col_name] - mean) / sd\n",
        "\n",
        "# Print last few rows of standardized training set\n",
        "df_train_std.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGvC0EzU3ptH"
      },
      "source": [
        "To simplify the learning task, let's collapse input `Model Year` into four equal-sized buckets: (-∞, 73), [73, 76), [76, 79), [79, ∞). Note that the cut-off values 73, 76, and 79 represent the first (Q1), second (Q2), and third (Q3) quartile of the distribution of feature `Model Year` (see below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEQj-7Ix3ptI",
        "outputId": "051569c5-ed4a-4a9b-ee5a-e08dae2a24eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    392.000000\n",
              "mean      75.979592\n",
              "std        3.683737\n",
              "min       70.000000\n",
              "25%       73.000000\n",
              "50%       76.000000\n",
              "75%       79.000000\n",
              "max       82.000000\n",
              "Name: Model Year, dtype: float64"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Descriptive statistics for column Model Year\n",
        "auto_mpg_df[\"Model Year\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1kvG7AC3ptI"
      },
      "source": [
        "We use the `torch.bucketize()` [function](https://pytorch.org/docs/stable/generated/torch.bucketize.html) from PyTorch to generate indices of the buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGFDF2aB3ptI"
      },
      "outputs": [],
      "source": [
        "# Import torch library\n",
        "import torch\n",
        "\n",
        "# Specify cut-off values (boundaries)\n",
        "boundaries = torch.tensor([73, 76, 79])\n",
        "\n",
        "# Extract values of feature Model Year and create tensor\n",
        "# for training and test sets\n",
        "v_train = torch.tensor(df_train_std[\"Model Year\"].values)\n",
        "\n",
        "v_test = torch.tensor(df_test_std[\"Model Year\"].values)\n",
        "\n",
        "# Use bucketize function to generate indices of buckets\n",
        "# and add as new feature to training and test sets\n",
        "df_train_std[\"Model Year Bucketed\"] = torch.bucketize(\n",
        "    v_train, boundaries, right=True  # if right is True, then right boundary is open (as defined in above text cell)\n",
        ")\n",
        "\n",
        "df_test_std[\"Model Year Bucketed\"] = torch.bucketize(\n",
        "    v_test, boundaries, right=True\n",
        ")\n",
        "\n",
        "# Add new features to list with names of numeric features\n",
        "numeric_col_names.append(\"Model Year Bucketed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhzQWymY3ptI"
      },
      "source": [
        "Recall that the feature `Origin` is of type unordered categorical. We will one-hot-encode this feature (i.e., create a set of indicator variables representing the feature categories), using the `one_hot()` [function](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html) from PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL9jt6wg3ptI"
      },
      "outputs": [],
      "source": [
        "# Import one_hot function\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "# Create a set containing the values in feature Origin,\n",
        "# return the number of items in the set, and store this\n",
        "# number in an object\n",
        "total_origin = len(set(df_train_std[\"Origin\"]))\n",
        "\n",
        "# One-hot-encode feature Origin for training and test sets\n",
        "origin_encoded_train = one_hot(\n",
        "    torch.from_numpy(df_train_std[\"Origin\"].values) % total_origin  # modulo operation turns set of class values {1,2,3} into {0,1,2}\n",
        ")\n",
        "\n",
        "origin_encoded_test = one_hot(\n",
        "    torch.from_numpy(df_test_std[\"Origin\"].values) % total_origin\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiQ08v_v3ptJ"
      },
      "source": [
        "We now concatenate the one-hot-encoded feature `Origin` with the numeric features for both the training and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ca95RtG3ptJ"
      },
      "outputs": [],
      "source": [
        "# Extract numeric features in training set and create tensor\n",
        "x_train_numeric = torch.tensor(\n",
        "    df_train_std[numeric_col_names].values\n",
        ")\n",
        "\n",
        "# Concatenate numeric features and encoded origin for training set\n",
        "x_train = torch.cat([x_train_numeric, origin_encoded_train], 1).float()  # dim=1 indicates dimension over which the tensors are concatenated: add 1 column\n",
        "\n",
        "# Extract numeric features in test set and create tensor\n",
        "x_test_numeric = torch.tensor(\n",
        "    df_test_std[numeric_col_names].values\n",
        ")\n",
        "\n",
        "# Concatenate numeric features and encoded origin for test set\n",
        "x_test = torch.cat([x_test_numeric, origin_encoded_test], 1).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Qa8NoB3ptJ"
      },
      "source": [
        "Finally, we create tensors from the output values for both the training and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmb84fAL3ptJ"
      },
      "outputs": [],
      "source": [
        "# Create output tensors for training and test sets\n",
        "y_train = torch.tensor(df_train_std[\"MPG\"].values).float()\n",
        "y_test = torch.tensor(df_test_std[\"MPG\"].values).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHc1ieaz3ptJ"
      },
      "source": [
        "### Training a DNN Regression Model <a class=\"anchor\" id=\"section_4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9kabitW3ptJ"
      },
      "source": [
        "We first create a data loader that uses a batch size of 8 for the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT0GG4r-3ptK"
      },
      "outputs": [],
      "source": [
        "# Import TensorDataset and Dataloader\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Combine input and output tensors in a joint training data set\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "\n",
        "# Specify batch size\n",
        "batch_size = 8\n",
        "\n",
        "# Create data loader\n",
        "torch.manual_seed(1)\n",
        "train_dl = DataLoader(\n",
        "    train_ds,  # data set from which to load data\n",
        "    batch_size,  # how many examples per batch to load\n",
        "    shuffle=True  # if True, then data are reshuffled at every epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snvuIESv3ptK"
      },
      "source": [
        "Next, we will build a deep neural network with two hidden layers using the `torch.nn` [module](https://pytorch.org/docs/stable/nn.html) from PyTorch. The first hidden layer has 8 hidden units and the second hidden layer has 4 hidden units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZzlI4Ls3ptK",
        "outputId": "d023e341-e2df-42b1-c9df-017fcaa17549"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=9, out_features=8, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=8, out_features=4, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=4, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import torch.nn module\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create list specifying number of hidden units per hidden layer\n",
        "hidden_units = [8, 4] # first layer 8 units, second 4\n",
        "\n",
        "# Initialize input size\n",
        "# (for the first hidden layer, the input size is the number of input features)\n",
        "input_size = x_train.shape[1]\n",
        "\n",
        "# Create container for the layers of the network\n",
        "all_layers = []\n",
        "\n",
        "# Create the layers of the network\n",
        "# (the loop first creates the two hidden layers\n",
        "for hidden_unit in hidden_units:\n",
        "    layer = nn.Linear(input_size, hidden_unit)  # nn.Linear applies a linear transformation to the incoming data; the first argument specifies input size, and the second argument specifies output size\n",
        "    all_layers.append(layer)  # add layer to container\n",
        "    all_layers.append(nn.ReLU())  # applies ReLU activation function\n",
        "    input_size = hidden_unit  # set input size of next layer equal to output size of current layer\n",
        "all_layers.append(nn.Linear(hidden_units[-1], 1))  # create the output layer; # of input is the output of last layer, and output is just one value, y; if more than 1, y is a scalar vector\n",
        "\n",
        "# Create the model based on the layers defined above\n",
        "model = nn.Sequential(*all_layers)  # Sequential connects the layers in a cascading way (all_layers is a list and the * operator that o;unpacks this list)\n",
        "\n",
        "# Print the model\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oebteQeK3ptK"
      },
      "source": [
        "After defining the model, we choose a loss function and an optimization algorithm. We will use here the mean squared error (MSE) as the loss function and stochastic gradient descent (SGD) as the optimization algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Jo2YDng3ptK"
      },
      "outputs": [],
      "source": [
        "# Specify MSE as loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Specify SGD as optimization algorithm\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)  # the first argument specifies parameters to optimize, the second argument specifies the learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1FsuGUh3ptL"
      },
      "source": [
        "We can now train the model. We use 200 epochs (passes over the full training data set) and display the training loss for every 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ati096d33ptL",
        "outputId": "51c6f572-7d34-40c9-91e5-713b69619291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:  Loss 536.1047\n",
            "Epoch 20:  Loss 8.4361\n",
            "Epoch 40:  Loss 7.8695\n",
            "Epoch 60:  Loss 7.1891\n",
            "Epoch 80:  Loss 6.7064\n",
            "Epoch 100:  Loss 6.7603\n",
            "Epoch 120:  Loss 6.3107\n",
            "Epoch 140:  Loss 6.6884\n",
            "Epoch 160:  Loss 6.7549\n",
            "Epoch 180:  Loss 6.2029\n"
          ]
        }
      ],
      "source": [
        "# Set seed\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Specify number of epochs\n",
        "num_epochs = 200\n",
        "\n",
        "# Specify after how many epochs loss is to be displayed\n",
        "log_epochs = 20\n",
        "\n",
        "# Train model\n",
        "for epoch in range(num_epochs):  # loop over epochs\n",
        "    loss_hist_train = 0  # Initialize loss history\n",
        "    for x_batch, y_batch in train_dl:  # loop over batches of training data\n",
        "        pred = model(x_batch)[:, 0]  # prediction of model for current batch\n",
        "        loss = loss_fn(pred, y_batch)  # loss for current batch\n",
        "        loss.backward()  # compute gradients with backpropagation\n",
        "        optimizer.step()  # update parameters based on computed gradients\n",
        "        optimizer.zero_grad()  # reset gradients to 0 (so that we do not add up the gradients)\n",
        "        loss_hist_train += loss.item()  # add current loss to loss history\n",
        "    if epoch % log_epochs==0:\n",
        "        print(f\"Epoch {epoch}:  Loss \"\n",
        "              f\"{loss_hist_train/len(train_dl):.4f}\")  # calculate average loss per batch for the epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYjuho-q3ptL"
      },
      "source": [
        "We can now evaluate the performance of the trained model on the test data set. To predict the outputs of new test examples, we feed their inputs to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6HJi4yT3ptL",
        "outputId": "1a94700e-f173-4c7f-e3ae-13426cbad137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test MSE: 9.5907\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():  # diables gradient calculation (since we are not anymore training the model) # even if I don't disable, pred doesn't compute gradient anymore actually, but just to be on the safe side, and also save memory\n",
        "    pred = model(x_test)[:, 0]\n",
        "    loss = loss_fn(pred, y_test)\n",
        "    print(f\"Test MSE: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOJBgafT3ptL"
      },
      "source": [
        "We now have an estimate of the expected test error of the model."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}